{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 340 Lecture 33: Deep learning software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming languages\n",
    "\n",
    "- In ML we use a variety of programming languages: Python, Java, R, C++, Matlab, many more. \n",
    "  - In CPSC 340 we used Python so I'll mainly stick to Python packages in this lecture.\n",
    "  - Most deep learning packages are Python-based in any case.\n",
    "\n",
    "- A popular open-source library is [scikit-learn](http://scikit-learn.org/stable/), which we've used a few times in the course. \n",
    "\n",
    "- In Assignment 6 you'll use sklearn's [neural network implementation](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html). By the way, this was [written by your TA, Issam](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/neural_network/multilayer_perceptron.py#L4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning software\n",
    "\n",
    "There's been a lot of software released lately to take care of this for you. Some big players are:\n",
    "\n",
    "| Name   |  Host language  | Released |  Comments | \n",
    "|--------|-------------|---------------|----------|\n",
    "| [Theano](http://deeplearning.net/software/theano/) | Python | 2007 | From U. de Montr√©al |\n",
    "| [Torch](http://torch.ch) | Lua | 2002 | Used at Facebook |\n",
    "| [PyTorch](http://pytorch.org) | Python | 2017 | Automatic differentiation through arbitrary code like [Autograd](https://github.com/HIPS/autograd)\n",
    "| [TensorFlow](https://www.tensorflow.org) | Python | 2015 | Created by Google for both prototyping and production\n",
    "| [Keras](https://keras.io) | Python | 2015 | A front-end on top of Theano or TensorFlow, soon to be more integrated |\n",
    "| [Caffe](http://caffe.berkeleyvision.org) | Executable with Python wrapper | 2014 | Specifically for convnets (see Lectures 31-32), from UC Berkeley\n",
    "\n",
    "- There are many others of course. See for example [this Wikipedia article](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software).\n",
    "- From the table above, we can see there have been a lot of new packages released recently.\n",
    "- Popular style is to define the model in the code rather than a config file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUs\n",
    "\n",
    "- Part of the recent progress in deep learning has been due to the computational power of GPUs.\n",
    "- GPUs were originally designed for graphics, which requires a lot of _matrix multiplication_.\n",
    "  - This is exactly what we need for neural networks.\n",
    "- The leader is NVIDIA, an American company, and their GPU programming language is called CUDA.\n",
    "  - Luckily, we rarely need to write CUDA code anymore, as there are intermediate layers of software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud computing\n",
    "\n",
    "- I'm guessing few/none of the laptops in the room have a CUDA-capable NVIDIA GPU capable of fast training. \n",
    "  - My Macbook Air certainly isn't good for much in terms of computation.\n",
    "- Luckily, the advent of cloud computing platforms like Amazon's EC2 gives us easy access to computing resources.\n",
    "- Amazon also offers [Amazon Machine Images](https://en.wikipedia.org/wiki/Amazon_Machine_Image) (AMIs) which simplify things further. \n",
    "  - This is like a virtual machine and comes with relevant software installed, etc.\n",
    "  - The AMI costs a bit extra on top of the EC2 costs (say 10%)\n",
    "  - If you were a big company you would probably create your own, but these public AMIs are great for prototyping.\n",
    "  - If you're interested in containerization, [Docker](https://www.docker.com/) is gaining a lot of popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demos: loading the data\n",
    "\n",
    "The data is built in to Keras, so we just access it for convenience. If not present already it is automatically downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train_cat), (X_test, y_test_cat) = mnist.load_data()\n",
    "\n",
    "img_dim = (28,28) \n",
    "img_size = img_dim[0]*img_dim[1]\n",
    "num_classes = 10\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test  = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test  /= 255\n",
    "X_train_flat = X_train.reshape(60000, img_size)\n",
    "X_test_flat  = X_test.reshape(10000, img_size)\n",
    "X_train = X_train[...,None] # add 4th dimension, needed later for convnets\n",
    "X_test  = X_test[...,None]\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train_cat, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test_cat, num_classes)\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: scikit-learn on my laptop\n",
    "\n",
    "See documentation for the [classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) and [regressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(100,100), max_iter=10, batch_size=128)\n",
    "nn.fit(X_train_flat, y_train_cat)\n",
    "\n",
    "score = nn.score(X_train_flat, y_train_cat)\n",
    "print('Train accuracy:', score)\n",
    "\n",
    "score = nn.score(X_test_flat, y_test_cat)\n",
    "print('Test accuracy:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Keras/TensorFlow on my laptop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected net\n",
    "\n",
    "Attribution: the code below is adapted from the [Keras MNIST example](https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py), which is under the [MIT license](https://github.com/fchollet/keras/blob/master/LICENSE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Model definition **\n",
    "\n",
    "Here we need to specify the input and output size in advance (unlike sklearn) because the model is first _compiled_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(X_train_flat.shape[1],), activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training and evaluation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "history = model.fit(X_train_flat, y_train,\n",
    "                    batch_size=128, \n",
    "                    nb_epoch=10,\n",
    "                    verbose=0)\n",
    "\n",
    "score = model.evaluate(X_train_flat, y_train, verbose=0)\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = model.evaluate(X_test_flat, y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional net\n",
    "\n",
    "Attribution: the code below is adapted from [Deep Learning with Python](https://machinelearningmastery.com/deep-learning-with-python2/) with permission from the author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Model definition **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 5, 5, input_shape=img_dim+(1,), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training and evaluation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=128, \n",
    "                    nb_epoch=1,\n",
    "                    verbose=0)\n",
    "\n",
    "score = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3: Keras/TensorFlow on EC2 (with GPU)\n",
    "\n",
    "- I'll now attempt to demo Keras/TensorFlow on EC2 \"from scratch\".\n",
    "  - By this I mean I haven't done much of the work in advance.\n",
    "  - You should be able to follow along if you already have an AWS account and you're willing to pay a few dollars.\n",
    "- This is probably a bad idea. I'll give the demo a 50% chance of working.\n",
    "  \n",
    "Here are the steps I'm about to do:\n",
    "\n",
    "1. (already done) sign into [AWS](https://aws.amazon.com).\n",
    "2. Go to the [AWS console](https://console.aws.amazon.com) and select EC2.\n",
    "3. Select \"Key Pairs\" on the left and create a new Key Pair.\n",
    "\n",
    "2. Visit the site of the [AMI I'm using](https://aws.amazon.com/marketplace/pp/B01EYKBEQ0).\n",
    "3. Press \"Continue\"\n",
    "4. Select US West (Oregon) as my [EC2 region](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html), and p2.xlarge as my [EC2 instance type](https://aws.amazon.com/ec2/instance-types/).\n",
    "5. Launch with 1-Click. The cost is \\$1/hour.\n",
    "5. Go back to the EC2 console and click Instances on the left side. \n",
    "6. Wait until the EC2 console lists the Instance State as \"running\"...\n",
    "7. Right-click the instance in the console, and press Connect. Follow the instructions there. \n",
    "  - change permissions on key\n",
    "  - `ssh` into the instance\n",
    "8. Run `watch -n 1 nvidia-smi` so I can monitor the GPU. You'll see it's a [Tesla K80](http://www.nvidia.com/object/tesla-k80.html), which is [not cheap](https://www.amazon.com/HP-J0G95A-NVIDIA-Tesla-K80/dp/B00TWFEIWA). It starts around body temperature.\n",
    "8. And now... go back to the console and get the public IP of the instance.\n",
    "9. Point my browser to `http://{EC2 Instance Public IP}:8888`\n",
    "10. Grab the instance id from the EC2 console: this is the password.\n",
    "11. Create a notebook, paste in the above code (data loading, model spec, fitting). And run!\n",
    "12. Go back to the terminal and see that the GPU is being utilized. Is the temperature going up?\n",
    "13. **IMPORTANT** Go back to the EC2 console, right-click on the instance, Instance State --> Terminate.\n",
    "\n",
    "Compared to my laptop, I observed a speedup of about **10x**. This translates into better models when training time is the limiting factor.\n",
    "\n",
    "Regarding the training and validation errors themselves, these aren't the most carefully tuned architectures. But the convnet run with ~10 epochs achieves 1% validation error which is not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: spot instances\n",
    "\n",
    "- EC2 offers both \"dedicated instances\" and [\"spot instances\"](https://aws.amazon.com/ec2/spot/). \n",
    "- Spot instances and much cheaper but may be interrupted if the \"spot price\" goes above your maximum price. \n",
    "  - For example, our instance is about \\$1/hour and a spot instance would be about \\$0.30/hour\n",
    "- In general you should always use spot instances when running experiments because of price.\n",
    "  - You'd want dedicated instances if you were _deploying_ your trained model in an app, though!\n",
    "- I am not using a spot instance in this demo because they aren't supported with the \"1-Click Launch\" feature of this AMI.\n",
    "  - But if I wasn't doing a demo then I'd launch a spot instance myself, through the EC2 console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Amazon educate\n",
    "\n",
    "- Amazon has a program that gives [free credit for students](https://aws.amazon.com/education/awseducate/). \n",
    "  - However, because UBC uses `@alumni.ubc.ca` email addresses for current students, the automated system tends to reject applications from UBC students.\n",
    "- We are working on this, maybe it will be solved by next year...\n",
    "  - If you have a CS ugrad account you can probably use it in the meantime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final advice: terminate your EC2 instances!!!\n",
    "\n",
    "- If you forget to do this, you can be stuck with a big bill from Amazon.\n",
    "- You can do this from the EC2 console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
